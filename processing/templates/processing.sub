# ==============================================================================
# processing.sub - HTCondor submit file for MC processing chain
# ==============================================================================
# This template is used by dag_generator.py to submit processing jobs.
# Runs the full chain: Shower -> Mix -> GEN-SIM -> RAW -> RECO -> MiniAOD -> Ntuple
#
# Required variables (set via DAGMan VARS):
#   campaign        - Campaign name (e.g., JJP_DPS1)
#   job_id          - Job identifier
#   inputs          - Comma-separated pool:index pairs
#   modes           - Comma-separated shower modes (normal|phi)
#   analysis        - Analysis type (JJP or JUP)
#   n_sources       - Number of input sources
# ==============================================================================

Universe = vanilla

# Executable and arguments
Executable = /afs/cern.ch/user/x/xcheng/condor/MC_Production_DAG/T2_CN_Beijing/processing/run_chain.sh
Arguments = --inputs $(inputs) --modes $(modes) --analysis $(analysis) --campaign $(campaign) --job-id $(job_id)

# Input files to transfer (self-contained sandbox)
Transfer_Input_Files = /afs/cern.ch/user/x/xcheng/condor/MC_Production_DAG/T2_CN_Beijing/processing/run_chain.sh, \
					   /afs/cern.ch/user/x/xcheng/condor/MC_Production_DAG/T2_CN_Beijing/processing/pythia_shower, \
					   /afs/cern.ch/user/x/xcheng/condor/MC_Production_DAG/T2_CN_Beijing/common

# Output handling
# We use XRootD stage-out to T2 storage, so don't transfer any output files back
# This avoids the 1GB transfer limit and saves AFS space
# stdout/stderr are handled separately by HTCondor via Output/Error directives
transfer_output_files =

# Log files
Output = log/proc_$(campaign)_$(job_id)_$(Cluster)_$(Process).stdout
Error = log/proc_$(campaign)_$(job_id)_$(Cluster)_$(Process).stderr
Log = log/proc_$(campaign)_$(job_id)_$(Cluster)_$(Process).log

# Resource requirements
# Full chain is very resource intensive
request_cpus = 8
request_memory = 20GB
request_disk = 50GB

# Wall time limit - full MC chain needs many hours
# Set to 3 days (259200 seconds) as requested
+MaxRuntime = 259200

# Target T2_CN_Beijing site
+DESIRED_Sites = "T2_CN_Beijing"

# X509 proxy for XRootD storage access
x509userproxy = /afs/cern.ch/user/x/xcheng/x509up_u180107
use_x509userproxy = true

# Run in CMSSW-compatible container
+SingularityImage = "/cvmfs/unpacked.cern.ch/registry.hub.docker.com/cmssw/el8:x86_64"

# Environment setup - use actual user ID for proxy path
Environment = "HOME=/srv X509_USER_PROXY=/afs/cern.ch/user/x/xcheng/x509up_u180107"

# Retry configuration
MaxRetries = 2
OnExitHold = (ExitCode != 0)
OnExitHoldReason = "Job exited with non-zero status"
OnExitHoldSubCode = 1

# Periodic release of held jobs
PeriodicRelease = (JobRunCount < 2) && (HoldReasonCode == 3)

# No host OS requirement - we run inside Singularity el8 container
# Container runs fine on AlmaLinux9 hosts

# Queue single job
Queue 1
